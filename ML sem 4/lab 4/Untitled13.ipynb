{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01cef59-aa2b-49b8-ab4c-6cac83e0f008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>20</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>21</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>19</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>at_home</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "390     MS   M   20       U     LE3       A     2     2  services  services   \n",
       "391     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "392     MS   M   21       R     GT3       T     1     1     other     other   \n",
       "393     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "394     MS   M   19       U     LE3       T     1     1     other   at_home   \n",
       "\n",
       "     ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0    ...      4        3      4     1     1      3        6   5   6   6  \n",
       "1    ...      5        3      3     1     1      3        4   5   5   6  \n",
       "2    ...      4        3      2     2     3      3       10   7   8  10  \n",
       "3    ...      3        2      2     1     1      5        2  15  14  15  \n",
       "4    ...      4        3      2     1     2      5        4   6  10  10  \n",
       "..   ...    ...      ...    ...   ...   ...    ...      ...  ..  ..  ..  \n",
       "390  ...      5        5      4     4     5      4       11   9   9   9  \n",
       "391  ...      2        4      5     3     4      2        3  14  16  16  \n",
       "392  ...      5        5      3     3     3      3        3  10   8   7  \n",
       "393  ...      4        4      1     3     4      5        0  11  12  10  \n",
       "394  ...      3        2      3     3     3      5        5   8   9   9  \n",
       "\n",
       "[395 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing \n",
    "df=pd.read_csv(\"C:/Users/rajpu/OneDrive/Desktop/b/student-mat.csv\",sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d83e557-fd04-4b0a-8d46-5f14368ea04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               G2          G3\n",
      "count  395.000000  395.000000\n",
      "mean    10.713924   10.415190\n",
      "std      3.761505    4.581443\n",
      "min      0.000000    0.000000\n",
      "25%      9.000000    8.000000\n",
      "50%     11.000000   11.000000\n",
      "75%     13.000000   14.000000\n",
      "max     19.000000   20.000000\n",
      "Shape of X: (395, 1)\n",
      "Shape of Y: (395,)\n",
      "   G2  G3\n",
      "0   6   6\n",
      "1   5   6\n",
      "2   8  10\n",
      "3  14  15\n",
      "4  10  10\n",
      "Weights: [-0.89607777  1.06079477]\n",
      "Final Loss: 3.820991140283462\n"
     ]
    }
   ],
   "source": [
    "XY = df.loc[:, ['G2', 'G3']]\n",
    "XY\n",
    "\n",
    "print(XY.describe())\n",
    "X = XY.iloc[:, :-1]\n",
    "Y = XY.iloc[:, -1]\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "\n",
    "\n",
    "X.insert(0, 'Ones', 1)\n",
    "print(XY.head())\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "Y = Y.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "XY = pd.concat([X, Y], axis=1).dropna()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict(weights, input_data):\n",
    "    return np.dot(input_data, weights)\n",
    "\n",
    "def calculate_loss(weights, input_data, output_data):\n",
    "    predictions = predict(weights, input_data)\n",
    "    return np.mean((predictions - output_data) ** 2)\n",
    "\n",
    "def calculate_gradient(weights, input_data, output_data):\n",
    "    predictions = predict(weights, input_data)\n",
    "    errors = predictions - output_data\n",
    "    gradient = np.dot(input_data.T, errors) / len(output_data)\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent(input_data, output_data, learning_rate, max_iterations):\n",
    "    weights = np.zeros(input_data.shape[1])\n",
    "    losses = []\n",
    "    for _ in range(max_iterations):\n",
    "        gradient = calculate_gradient(weights, input_data, output_data)\n",
    "        weights -= learning_rate * gradient\n",
    "        loss = calculate_loss(weights, input_data, output_data)\n",
    "        losses.append(loss)\n",
    "    return weights, losses\n",
    "\n",
    "max_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "weights, losses = gradient_descent(X.values, Y.values, learning_rate, max_iterations)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Final Loss:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59ea596-cbd5-4d89-84fb-be44fa300ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Text using CountVectorizer:\n",
      "[[ 3  1  1  1  1  4  1  1  1  1  4  1  2  1  1  1  1  1  1  1  3  2  1  1\n",
      "   2  2  1  2  2  1  2  1  2  2  1  2  2  1  1  1  2  1  1  2  1  2  2  1\n",
      "   1 13  2  1  1  2  1  1  2  1  1  1  1  2  5  2  1  1  1  1  2  1  9  2\n",
      "   1  1  1  3  2  3  1  1  1  3  1  1  1  1  1  1  2  1  1  1 11  1  1  1\n",
      "   1  1  1  1  1  7  1  1  1  2  4  4  4  4  2  4  1  1  1  1  2  2  1  1\n",
      "   1  1  4  1  1  3  1  3  1  3  1  1  8  4  2  1  2 16  2  3 26  4  1  1\n",
      "   2  1  1  1  2  2  1  3  1  2  1  1  2  3  1  1  1  2  1  8  1  1  1  2\n",
      "   2  1  2  1  1  1  2  8  2  1  1  2  2  1  1  2  1  2  3  1  2  1  3 23\n",
      "   1  1  1  1  1 11  1  1  1  1  3  1  1  8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open(\"student.txt\", \"r\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "encoded_text = count_vectorizer.fit_transform([text_data])\n",
    "\n",
    "print(\"Encoded Text using CountVectorizer:\")\n",
    "print(encoded_text.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70ced0ca-aba9-4da8-8c09-8eccdf9f5c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '20' '21' '22' '23'\n",
      " '24' '25' '26' '27' '28' '29' '30' '31' '32' '382' '4th' '5th' '93' '9th'\n",
      " 'absences' 'access' 'activities' 'additional' 'address' 'administrative'\n",
      " 'after' 'age' 'alcohol' 'and' 'annexed' 'apart' 'are' 'as' 'at' 'at_home'\n",
      " 'attended' 'attributes' 'bad' 'be' 'belong' 'binary' 'both' 'by' 'can'\n",
      " 'care' 'characterize' 'choose' 'civil' 'class' 'classes' 'close'\n",
      " 'cohabitation' 'consumption' 'course' 'csv' 'current' 'curricular' 'da'\n",
      " 'dalc' 'datasets' 'each' 'education' 'educational' 'else' 'equal'\n",
      " 'excellent' 'extra' 'failures' 'family' 'famrel' 'famsize' 'famsup'\n",
      " 'father' 'fedu' 'female' 'file' 'final' 'first' 'fjob' 'for' 'free'\n",
      " 'freetime' 'friends' 'from' 'g1' 'g2' 'g3' 'gabriel' 'going' 'good'\n",
      " 'goout' 'gp' 'grade' 'grades' 'greater' 'gt3' 'guardian' 'health' 'high'\n",
      " 'higher' 'home' 'hour' 'hours' 'identical' 'identified' 'if' 'in'\n",
      " 'internet' 'job' 'language' 'le3' 'less' 'living' 'low' 'male' 'mat'\n",
      " 'math' 'medu' 'min' 'mjob' 'mother' 'mousinho' 'ms' 'no' 'nominal' 'none'\n",
      " 'note' 'number' 'numeric' 'nursery' 'of' 'or' 'other' 'out' 'output'\n",
      " 'paid' 'parent' 'past' 'pereira' 'period' 'police' 'por' 'portuguese'\n",
      " 'preference' 'primary' 'pstatus' 'quality' 'reason' 'related'\n",
      " 'relationship' 'relationships' 'reputation' 'romantic' 'rural' 'school'\n",
      " 'schoolsup' 'searching' 'second' 'secondary' 'services' 'several' 'sex'\n",
      " 'shown' 'silveira' 'size' 'status' 'student' 'students' 'study'\n",
      " 'studytime' 'subject' 'support' 'take' 'target' 'teacher' 'than' 'that'\n",
      " 'the' 'there' 'these' 'this' 'time' 'to' 'together' 'travel' 'traveltime'\n",
      " 'type' 'urban' 'very' 'walc' 'wants' 'weekend' 'weekly' 'with' 'within'\n",
      " 'workday' 'yes']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\")\n",
    "print(count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16665d41-fdb3-426a-9759-50083311a428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Text using TF-IDF:\n",
      "[[0.05591141 0.01863714 0.01863714 0.01863714 0.01863714 0.07454854\n",
      "  0.01863714 0.01863714 0.01863714 0.01863714 0.07454854 0.01863714\n",
      "  0.03727427 0.01863714 0.01863714 0.01863714 0.01863714 0.01863714\n",
      "  0.01863714 0.01863714 0.05591141 0.03727427 0.01863714 0.01863714\n",
      "  0.03727427 0.03727427 0.01863714 0.03727427 0.03727427 0.01863714\n",
      "  0.03727427 0.01863714 0.03727427 0.03727427 0.01863714 0.03727427\n",
      "  0.03727427 0.01863714 0.01863714 0.01863714 0.03727427 0.01863714\n",
      "  0.01863714 0.03727427 0.01863714 0.03727427 0.03727427 0.01863714\n",
      "  0.01863714 0.24228276 0.03727427 0.01863714 0.01863714 0.03727427\n",
      "  0.01863714 0.01863714 0.03727427 0.01863714 0.01863714 0.01863714\n",
      "  0.01863714 0.03727427 0.09318568 0.03727427 0.01863714 0.01863714\n",
      "  0.01863714 0.01863714 0.03727427 0.01863714 0.16773422 0.03727427\n",
      "  0.01863714 0.01863714 0.01863714 0.05591141 0.03727427 0.05591141\n",
      "  0.01863714 0.01863714 0.01863714 0.05591141 0.01863714 0.01863714\n",
      "  0.01863714 0.01863714 0.01863714 0.01863714 0.03727427 0.01863714\n",
      "  0.01863714 0.01863714 0.20500849 0.01863714 0.01863714 0.01863714\n",
      "  0.01863714 0.01863714 0.01863714 0.01863714 0.01863714 0.13045995\n",
      "  0.01863714 0.01863714 0.01863714 0.03727427 0.07454854 0.07454854\n",
      "  0.07454854 0.07454854 0.03727427 0.07454854 0.01863714 0.01863714\n",
      "  0.01863714 0.01863714 0.03727427 0.03727427 0.01863714 0.01863714\n",
      "  0.01863714 0.01863714 0.07454854 0.01863714 0.01863714 0.05591141\n",
      "  0.01863714 0.05591141 0.01863714 0.05591141 0.01863714 0.01863714\n",
      "  0.14909709 0.07454854 0.03727427 0.01863714 0.03727427 0.29819417\n",
      "  0.03727427 0.05591141 0.48456553 0.07454854 0.01863714 0.01863714\n",
      "  0.03727427 0.01863714 0.01863714 0.01863714 0.03727427 0.03727427\n",
      "  0.01863714 0.05591141 0.01863714 0.03727427 0.01863714 0.01863714\n",
      "  0.03727427 0.05591141 0.01863714 0.01863714 0.01863714 0.03727427\n",
      "  0.01863714 0.14909709 0.01863714 0.01863714 0.01863714 0.03727427\n",
      "  0.03727427 0.01863714 0.03727427 0.01863714 0.01863714 0.01863714\n",
      "  0.03727427 0.14909709 0.03727427 0.01863714 0.01863714 0.03727427\n",
      "  0.03727427 0.01863714 0.01863714 0.03727427 0.01863714 0.03727427\n",
      "  0.05591141 0.01863714 0.03727427 0.01863714 0.05591141 0.42865412\n",
      "  0.01863714 0.01863714 0.01863714 0.01863714 0.01863714 0.20500849\n",
      "  0.01863714 0.01863714 0.01863714 0.01863714 0.05591141 0.01863714\n",
      "  0.01863714 0.14909709]]\n",
      "Vocabulary:\n",
      "['10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '20' '21' '22' '23'\n",
      " '24' '25' '26' '27' '28' '29' '30' '31' '32' '382' '4th' '5th' '93' '9th'\n",
      " 'absences' 'access' 'activities' 'additional' 'address' 'administrative'\n",
      " 'after' 'age' 'alcohol' 'and' 'annexed' 'apart' 'are' 'as' 'at' 'at_home'\n",
      " 'attended' 'attributes' 'bad' 'be' 'belong' 'binary' 'both' 'by' 'can'\n",
      " 'care' 'characterize' 'choose' 'civil' 'class' 'classes' 'close'\n",
      " 'cohabitation' 'consumption' 'course' 'csv' 'current' 'curricular' 'da'\n",
      " 'dalc' 'datasets' 'each' 'education' 'educational' 'else' 'equal'\n",
      " 'excellent' 'extra' 'failures' 'family' 'famrel' 'famsize' 'famsup'\n",
      " 'father' 'fedu' 'female' 'file' 'final' 'first' 'fjob' 'for' 'free'\n",
      " 'freetime' 'friends' 'from' 'g1' 'g2' 'g3' 'gabriel' 'going' 'good'\n",
      " 'goout' 'gp' 'grade' 'grades' 'greater' 'gt3' 'guardian' 'health' 'high'\n",
      " 'higher' 'home' 'hour' 'hours' 'identical' 'identified' 'if' 'in'\n",
      " 'internet' 'job' 'language' 'le3' 'less' 'living' 'low' 'male' 'mat'\n",
      " 'math' 'medu' 'min' 'mjob' 'mother' 'mousinho' 'ms' 'no' 'nominal' 'none'\n",
      " 'note' 'number' 'numeric' 'nursery' 'of' 'or' 'other' 'out' 'output'\n",
      " 'paid' 'parent' 'past' 'pereira' 'period' 'police' 'por' 'portuguese'\n",
      " 'preference' 'primary' 'pstatus' 'quality' 'reason' 'related'\n",
      " 'relationship' 'relationships' 'reputation' 'romantic' 'rural' 'school'\n",
      " 'schoolsup' 'searching' 'second' 'secondary' 'services' 'several' 'sex'\n",
      " 'shown' 'silveira' 'size' 'status' 'student' 'students' 'study'\n",
      " 'studytime' 'subject' 'support' 'take' 'target' 'teacher' 'than' 'that'\n",
      " 'the' 'there' 'these' 'this' 'time' 'to' 'together' 'travel' 'traveltime'\n",
      " 'type' 'urban' 'very' 'walc' 'wants' 'weekend' 'weekly' 'with' 'within'\n",
      " 'workday' 'yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data using TF-IDF\n",
    "encoded_text_tfidf = tfidf_vectorizer.fit_transform([text_data])\n",
    "\n",
    "# Display the encoded textual information using TF-IDF\n",
    "print(\"\\nEncoded Text using TF-IDF:\")\n",
    "print(encoded_text_tfidf.toarray())\n",
    "print(\"Vocabulary:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe7c853-bacf-47d7-baae-98c24906788e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Text using HashingVectorizer:\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Initialize HashingVectorizer\n",
    "hashing_vectorizer = HashingVectorizer()\n",
    "\n",
    "# Transform the text data using HashingVectorizer\n",
    "encoded_text_hashing = hashing_vectorizer.transform([text_data])\n",
    "\n",
    "# Display the encoded textual information using HashingVectorizer\n",
    "print(\"\\nEncoded Text using HashingVectorizer:\")\n",
    "print(encoded_text_hashing.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b065512a-3b32-40d3-bcf0-59edbbac172e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b04f2-ecd1-43e6-aed2-0d03c0eabd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
